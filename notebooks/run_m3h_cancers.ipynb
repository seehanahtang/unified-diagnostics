{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec07b6e-0f91-4da9-8087-9667dc5ccd44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run_m3h_cancers.py\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import prune\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from M3H import M3H, train_with_early_stopping\n",
    "\n",
    "# ----------------------------\n",
    "# Config (edit these)\n",
    "# ----------------------------\n",
    "CANCER_CSV = \"data/blood_protein_cancers_clean.csv\"     # change\n",
    "ID_COL    = \"eid\"                          # change\n",
    "CANCER_LABELS = [\n",
    "    \"breast_cancer\", \"prostate_cancer\", \"lung_cancer\", \"colorectal_cancer\", \n",
    "    \"bladder_cancer\", \"pancreatic_cancer\", \"liver_cancer\"\n",
    "]\n",
    "SAVE_DIR = \"output/m3h_cancers\"; os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "SEED=42; np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "BATCH_SIZE=128; MAX_EPOCHS=200; PATIENCE=20; LR=3e-4; WD=1e-4\n",
    "VAL_FRAC=0.15; TEST_FRAC=0.15\n",
    "\n",
    "# ----------------------------\n",
    "# Data utils\n",
    "# ----------------------------\n",
    "def split(df, ycols):\n",
    "    df = df.dropna(subset=ycols)\n",
    "    # stratify on \"has any cancer\" to stabilize splits\n",
    "    strat = (df[ycols].sum(axis=1) > 0).astype(int)\n",
    "    rest, test = train_test_split(df, test_size=TEST_FRAC, random_state=SEED, stratify=strat)\n",
    "    strat_rest = (rest[ycols].sum(axis=1) > 0).astype(int)\n",
    "    val_size = VAL_FRAC/(1-TEST_FRAC)\n",
    "    train, val = train_test_split(rest, test_size=val_size, random_state=SEED, stratify=strat_rest)\n",
    "    return train, val, test\n",
    "\n",
    "def matrices(df, ycols, id_col, feature_prefixes = [\"blood_\", \"olink_\"]):\n",
    "    feature_cols = [\n",
    "        c for c in df.columns\n",
    "        if any(c.startswith(pref) for pref in feature_prefixes)\n",
    "    ]\n",
    "    X = df[feature_cols].astype(np.float32).values\n",
    "    y = df[ycols].astype(np.float32).values\n",
    "    return X, y\n",
    "\n",
    "class TabularSet(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "# def main():\n",
    "#     df = pd.read_csv(CANCER_CSV)\n",
    "#     tr_df, va_df, te_df = split(df, CANCER_LABELS)\n",
    "\n",
    "#     Xtr, Ytr = matrices(tr_df, CANCER_LABELS, ID_COL)\n",
    "#     Xva, Yva = matrices(va_df, CANCER_LABELS, ID_COL)\n",
    "#     Xte, Yte = matrices(te_df, CANCER_LABELS, ID_COL)\n",
    "\n",
    "#     scaler = StandardScaler().fit(Xtr)\n",
    "#     Xtr = scaler.transform(Xtr).astype(np.float32)\n",
    "#     Xva = scaler.transform(Xva).astype(np.float32)\n",
    "#     Xte = scaler.transform(Xte).astype(np.float32)\n",
    "\n",
    "#     dl_tr = DataLoader(TabularSet(Xtr, Ytr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "#     dl_va = DataLoader(TabularSet(Xva, Yva), batch_size=BATCH_SIZE, shuffle=False)\n",
    "#     dl_te = DataLoader(TabularSet(Xte, Yte), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "#     # --- Model: y1_bins = 7 tasks (one per cancer)\n",
    "#     model = M3H(\n",
    "#         input_dim=Xtr.shape[1],\n",
    "#         y1_bins=7,              # 7 parallel heads/tasks\n",
    "#         alpha=1.0,              # task attention mixing strength\n",
    "#         hidden_dim=128,         # project feature subsets -> hidden_dim\n",
    "#         hidden_layers=2,        # per-head depth\n",
    "#         feature_indices_per_head=[list(range(Xtr.shape[1])) for _ in range(7)],\n",
    "#         prune_amount=0.0        # turn off pruning unless you want it\n",
    "#     )\n",
    "\n",
    "#     opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "#     # --- Use your built-in trainer (early stopping + best-state restore)\n",
    "#     train_with_early_stopping(\n",
    "#         model=model,\n",
    "#         train_loader=dl_tr,\n",
    "#         val_loader=dl_va,\n",
    "#         optimizer=opt,\n",
    "#         num_epochs=MAX_EPOCHS,\n",
    "#         patience=PATIENCE,\n",
    "#         l1=1e-4,               # L1 on head params (supported by your trainer)\n",
    "#     )\n",
    "\n",
    "#     # --- Evaluate (AUROC / AP per task)\n",
    "#     model.eval()\n",
    "#     probs, targs = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in dl_te:\n",
    "#             p = model(xb)              # [B,7] probabilities from your forward\n",
    "#             probs.append(p.cpu().numpy())\n",
    "#             targs.append(yb.cpu().numpy())\n",
    "#     P = np.vstack(probs); Y = np.vstack(targs)\n",
    "\n",
    "#     metrics = {}\n",
    "#     aurocs, aps = [], []\n",
    "#     for j, name in enumerate(CANCER_LABELS):\n",
    "#         if len(np.unique(Y[:, j])) < 2:\n",
    "#             continue\n",
    "#         au = roc_auc_score(Y[:, j], P[:, j])\n",
    "#         ap = average_precision_score(Y[:, j], P[:, j])\n",
    "#         metrics[f\"{name}_AUROC\"] = float(au)\n",
    "#         metrics[f\"{name}_AP\"]    = float(ap)\n",
    "#         aurocs.append(au); aps.append(ap)\n",
    "#     if aurocs:\n",
    "#         metrics[\"macro_AUROC\"] = float(np.mean(aurocs))\n",
    "#         metrics[\"macro_AP\"]    = float(np.mean(aps))\n",
    "\n",
    "#     # --- Save\n",
    "#     torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"m3h_7cancers.pt\"))\n",
    "#     np.save(os.path.join(SAVE_DIR, \"test_probs.npy\"), P)\n",
    "#     np.save(os.path.join(SAVE_DIR, \"test_targets.npy\"), Y)\n",
    "#     with open(os.path.join(SAVE_DIR, \"metrics.json\"), \"w\") as f: json.dump(metrics, f, indent=2)\n",
    "#     with open(os.path.join(SAVE_DIR, \"labels.json\"), \"w\") as f: json.dump(CANCER_LABELS, f, indent=2)\n",
    "\n",
    "#     print(json.dumps(metrics, indent=2))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bcea357-3146-4180-8447-4f546a741e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(CANCER_CSV)\n",
    "tr_df, va_df, te_df = split(df, CANCER_LABELS)\n",
    "\n",
    "Xtr, Ytr = matrices(tr_df, CANCER_LABELS, ID_COL)\n",
    "Xva, Yva = matrices(va_df, CANCER_LABELS, ID_COL)\n",
    "Xte, Yte = matrices(te_df, CANCER_LABELS, ID_COL)\n",
    "\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr = scaler.transform(Xtr).astype(np.float32)\n",
    "Xva = scaler.transform(Xva).astype(np.float32)\n",
    "Xte = scaler.transform(Xte).astype(np.float32)\n",
    "\n",
    "dl_tr = DataLoader(TabularSet(Xtr, Ytr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "dl_va = DataLoader(TabularSet(Xva, Yva), batch_size=BATCH_SIZE, shuffle=False)\n",
    "dl_te = DataLoader(TabularSet(Xte, Yte), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- Model: y1_bins = 7 tasks (one per cancer)\n",
    "model = M3H(\n",
    "    input_dim=Xtr.shape[1],\n",
    "    y1_bins=7,              # 7 parallel heads/tasks\n",
    "    alpha=1.0,              # task attention mixing strength\n",
    "    hidden_dim=128,         # project feature subsets -> hidden_dim\n",
    "    hidden_layers=2,        # per-head depth\n",
    "    feature_indices_per_head=[list(range(Xtr.shape[1])) for _ in range(7)],\n",
    "    prune_amount=0.0        # turn off pruning unless you want it\n",
    ")\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d526fe5-a3eb-40b8-a1ab-8d315fb15191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: nan | Val Loss: nan\n",
      "Epoch 2 | Train Loss: nan | Val Loss: nan\n",
      "Epoch 3 | Train Loss: nan | Val Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- Use your built-in trainer (early stopping + best-state restore)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_with_early_stopping(\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      4\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mdl_tr,\n\u001b[1;32m      5\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mdl_va,\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mopt,\n\u001b[1;32m      7\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mMAX_EPOCHS,\n\u001b[1;32m      8\u001b[0m     patience\u001b[38;5;241m=\u001b[39mPATIENCE,\n\u001b[1;32m      9\u001b[0m     l1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,               \u001b[38;5;66;03m# L1 on head params (supported by your trainer)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m/orcd/home/002/seehanah/diagnostics/ukbiobank/M3H.py:169\u001b[0m, in \u001b[0;36mtrain_with_early_stopping\u001b[0;34m(model, train_loader, val_loader, optimizer, num_epochs, patience, l1)\u001b[0m\n\u001b[1;32m    162\u001b[0m l1_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    163\u001b[0m     param\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters()\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    167\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l1 \u001b[38;5;241m*\u001b[39m l1_reg\n\u001b[0;32m--> 169\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    170\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    171\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/home/software/anaconda3/2023.07/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/home/software/anaconda3/2023.07/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Use your built-in trainer (early stopping + best-state restore)\n",
    "train_with_early_stopping(\n",
    "    model=model,\n",
    "    train_loader=dl_tr,\n",
    "    val_loader=dl_va,\n",
    "    optimizer=opt,\n",
    "    num_epochs=MAX_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    l1=1e-4,               # L1 on head params (supported by your trainer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3aa093-b38c-4deb-b0d3-f755d2690088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Use your built-in trainer (early stopping + best-state restore)\n",
    "train_with_early_stopping(\n",
    "    model=model,\n",
    "    train_loader=dl_tr,\n",
    "    val_loader=dl_va,\n",
    "    optimizer=opt,\n",
    "    num_epochs=MAX_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    l1=1e-4,               # L1 on head params (supported by your trainer)\n",
    ")\n",
    "\n",
    "# --- Evaluate (AUROC / AP per task)\n",
    "model.eval()\n",
    "probs, targs = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in dl_te:\n",
    "        p = model(xb)              # [B,7] probabilities from your forward\n",
    "        probs.append(p.cpu().numpy())\n",
    "        targs.append(yb.cpu().numpy())\n",
    "P = np.vstack(probs); Y = np.vstack(targs)\n",
    "\n",
    "metrics = {}\n",
    "aurocs, aps = [], []\n",
    "for j, name in enumerate(CANCER_LABELS):\n",
    "    if len(np.unique(Y[:, j])) < 2:\n",
    "        continue\n",
    "    au = roc_auc_score(Y[:, j], P[:, j])\n",
    "    ap = average_precision_score(Y[:, j], P[:, j])\n",
    "    metrics[f\"{name}_AUROC\"] = float(au)\n",
    "    metrics[f\"{name}_AP\"]    = float(ap)\n",
    "    aurocs.append(au); aps.append(ap)\n",
    "if aurocs:\n",
    "    metrics[\"macro_AUROC\"] = float(np.mean(aurocs))\n",
    "    metrics[\"macro_AP\"]    = float(np.mean(aps))\n",
    "\n",
    "# --- Save\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"m3h_7cancers.pt\"))\n",
    "np.save(os.path.join(SAVE_DIR, \"test_probs.npy\"), P)\n",
    "np.save(os.path.join(SAVE_DIR, \"test_targets.npy\"), Y)\n",
    "with open(os.path.join(SAVE_DIR, \"metrics.json\"), \"w\") as f: json.dump(metrics, f, indent=2)\n",
    "with open(os.path.join(SAVE_DIR, \"labels.json\"), \"w\") as f: json.dump(CANCER_LABELS, f, indent=2)\n",
    "\n",
    "print(json.dumps(metrics, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
