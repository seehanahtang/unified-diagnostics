{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47d4eeb1-91bd-4ed2-bf64-816ed7487db0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_10000_10500_wide.parquet number of columns:  364\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_10000_10500_wide.parquet number of columns:  364\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_10500_11000_wide.parquet number of columns:  38\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_10500_11000_wide.parquet number of columns:  38\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_191000_192000_wide.parquet number of columns:  674\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_191000_192000_wide.parquet number of columns:  674\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_192000_193000_wide.parquet number of columns:  429\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_192000_193000_wide.parquet number of columns:  429\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_368000_369000_wide.parquet number of columns:  401\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_368000_369000_wide.parquet number of columns:  401\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_369000_370000_wide.parquet number of columns:  308\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_369000_370000_wide.parquet number of columns:  308\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_9000_9500_wide.parquet number of columns:  35\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_9000_9500_wide.parquet number of columns:  35\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_9500_10000_wide.parquet number of columns:  328\n",
      "/orcd/pool/003/dbertsim_shared/ukb/bgen/ch18/wide_format/c18_b0_v1_9500_10000_wide.parquet number of columns:  328\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def get_sample_eids():\n",
    "    \"\"\"\n",
    "    Load train/valid CSVs and return their eids as lists.\n",
    "    Assumes preprocess_categorical() is defined elsewhere.\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(f\"{DATA_DIR}/ukb_cancer_train.csv\")\n",
    "    valid_df = pd.read_csv(f\"{DATA_DIR}/ukb_cancer_valid.csv\")\n",
    "    train_eids = list(train_df[\"eid\"])\n",
    "    valid_eids = list(valid_df[\"eid\"])\n",
    "    return train_df, valid_df, train_eids, valid_eids\n",
    "\n",
    "def get_df(path, samples, variants, chrom=18):\n",
    "    # 1. Read wide-genotype parquet\n",
    "    tbl = pq.read_table(path)\n",
    "    wide = tbl.to_pandas()   # columns: sample_idx, ch8_12345, ch8_67890, ...\n",
    "\n",
    "    # 2. Attach ID_1 (eid) via sample_idx\n",
    "    merged = wide.merge(\n",
    "        samples[[\"sample_idx\", \"ID_1\"]],\n",
    "        on=\"sample_idx\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # 3. Drop sample_idx and rename ID_1 -> eid\n",
    "    merged = merged.drop(columns=[\"sample_idx\"])\n",
    "    merged = merged.rename(columns={\"ID_1\": \"eid\"})\n",
    "\n",
    "    # 4. Build mapping from variant_idx -> rsid\n",
    "    #    (variants should be pre-filtered to your BED overlaps, etc.)\n",
    "    idx_to_rsid = dict(zip(variants[\"variant_idx\"], variants[\"rsid\"]))\n",
    "\n",
    "    # 5. Figure out which columns are genotype columns\n",
    "    prefix = f\"c{chrom}_\"\n",
    "    geno_cols = [c for c in merged.columns if c.startswith(prefix)]\n",
    "\n",
    "    # 6. Build renaming dict: ch8_12345 -> rsid_for_12345\n",
    "    col_rename = {}\n",
    "    for col in geno_cols:\n",
    "        try:\n",
    "            var_idx = int(col.split(\"_\")[1])  # from \"ch8_12345\" -> 12345\n",
    "        except (IndexError, ValueError):\n",
    "            continue\n",
    "\n",
    "        if var_idx in idx_to_rsid:\n",
    "            col_rename[col] = idx_to_rsid[var_idx]\n",
    "        # else: this variant not in your variants list â†’ drop it later\n",
    "\n",
    "    # 7. Keep only eid + the genotype columns that we know how to map\n",
    "    cols_keep = [\"eid\"] + list(col_rename.keys())\n",
    "    print(path, \"number of columns: \", len(cols_keep)-1)\n",
    "    final = merged[cols_keep].rename(columns=col_rename)\n",
    "\n",
    "    final = final.replace(-1, np.nan)\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "CHROM = 18\n",
    "OUTCOME = \"lung\" #breast, lung, prostate\n",
    "DATA_DIR = \"/orcd/pool/003/dbertsim_shared/ukb\"\n",
    "LOGFILE = f\"logs/xgb_chr{CHROM}_{OUTCOME}.log\"\n",
    "bgen_path = f\"{DATA_DIR}/bgen/ch{CHROM}\"\n",
    "file = f\"c{CHROM}_b0_v1\"\n",
    "\n",
    "# train_pheno, test_pheno, train_eids, test_eids = get_sample_eids()\n",
    "\n",
    "# samples = pd.read_csv(f\"{bgen_path}/c{CHROM}_b0_v1_samples.csv\")\n",
    "\n",
    "# train_samples = samples.loc[samples[\"ID_1\"].isin(train_eids)].copy()\n",
    "# test_samples = samples.loc[samples[\"ID_1\"].isin(test_eids)].copy()\n",
    "\n",
    "# # variants = pd.read_csv(f\"{DATA_DIR}/bed/variants_overlaps_{outcome}_cancer.csv\")\n",
    "# variants = pd.read_csv(f\"variants_overlaps_{OUTCOME}_cancer.csv\")\n",
    "variant_idxs = variants['variant_idx']\n",
    "\n",
    "parquet_files = sorted(glob.glob(os.path.join(bgen_path, f\"wide_format/c{CHROM}_*.parquet\")))\n",
    "train_chunks = []\n",
    "test_chunks = []\n",
    "\n",
    "for path in parquet_files:\n",
    "    start, end = re.findall(r\"_(\\d+)\", path)[-2:]\n",
    "    start = int(start)\n",
    "    end = int(end)\n",
    "    if any(start <= x <= end for x in variant_idxs):\n",
    "        train_chunk = get_df(path, train_samples, variants, CHROM)\n",
    "        test_chunk = get_df(path, test_samples, variants, CHROM)\n",
    "\n",
    "        # Use eid as index to guarantee consistent ordering when concatenating\n",
    "        train_chunks.append(train_chunk.set_index(\"eid\"))\n",
    "        test_chunks.append(test_chunk.set_index(\"eid\"))\n",
    "    \n",
    "X_train_geno = pd.concat(train_chunks, axis=1)\n",
    "X_test_geno = pd.concat(test_chunks, axis=1)\n",
    "\n",
    "X_train_geno = X_train_geno.reset_index()\n",
    "X_test_geno = X_test_geno.reset_index()\n",
    "\n",
    "# Merge with phenotype to align labels and features\n",
    "outcome_col = f\"{OUTCOME}_cancer\"\n",
    "\n",
    "train_merge = (\n",
    "    train_pheno[[\"eid\", outcome_col]]\n",
    "    .merge(X_train_geno, on=\"eid\", how=\"inner\")\n",
    ")\n",
    "valid_merge = (\n",
    "    test_pheno[[\"eid\", outcome_col]]\n",
    "    .merge(X_test_geno, on=\"eid\", how=\"inner\")\n",
    ")\n",
    "feature_cols = [c for c in train_merge.columns if c not in [\"eid\", outcome_col]]\n",
    "\n",
    "X_train = train_merge[feature_cols]\n",
    "y_train = train_merge[outcome_col].astype(int)\n",
    "\n",
    "X_valid = valid_merge[feature_cols]\n",
    "y_valid = valid_merge[outcome_col].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "664e9ba9-5710-4acb-ba52-254015aa521b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_merge.to_csv(f\"ukb_train_{outcome_col}_selected_variants.csv\",index=False)\n",
    "valid_merge.to_csv(f\"ukb_valid_{outcome_col}_selected_variants.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d4193-a4e6-4f2d-a69a-55a4590bc1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seehanah/.local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [13:42:51] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# XGBoost with your fixed hyperparameters\n",
    "model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    device=\"cuda\",\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.02,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    ")\n",
    "start_time = time.time()\n",
    "sys.stdout = open(LOGFILE, \"a\")\n",
    "\n",
    "print(f\"Training XGBoost on chr{CHROM}, outcome={OUTCOME}\")\n",
    "print(f\"n_train={len(X_train)}, n_valid={len(X_valid)}\")\n",
    "print(\"Fixed hyperparameters:\")\n",
    "print({\n",
    "    \"n_estimators\": 300,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "})\n",
    "print(\"========================================\")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Time taken: {(time.time() - start_time)/60:.2f} minutes\")\n",
    "print(\"========================================\")\n",
    "\n",
    "# ------------------ EVALUATE ------------------ #\n",
    "y_valid_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "auc_valid = roc_auc_score(y_valid, y_valid_pred_prob)\n",
    "print(f\"Validation AUC: {auc_valid:.4f}\")\n",
    "print(\"========================================\")\n",
    "\n",
    "sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbca031e-0ba4-44b1-b513-f710ccae6fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5500 features total:\n",
      "  2923 protein (Olink) features\n",
      "  2577 SNP features\n",
      "Fitting XGBoost model on combined protein + genomic features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seehanah/.local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [15:34:28] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 121\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# train_merged, valid_merged = load_data()\u001b[39;00m\n\u001b[1;32m    117\u001b[0m X_train, y_train, X_valid, y_valid, feature_cols \u001b[38;5;241m=\u001b[39m select_features(\n\u001b[1;32m    118\u001b[0m     train_merged, valid_merged\n\u001b[1;32m    119\u001b[0m )\n\u001b[0;32m--> 121\u001b[0m model, valid_pred_proba, auc \u001b[38;5;241m=\u001b[39m train_xgb(X_train, y_train, X_valid, y_valid)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Save predictions & feature list\u001b[39;00m\n\u001b[1;32m    124\u001b[0m pred_out_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_valid_preds.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[20], line 102\u001b[0m, in \u001b[0;36mtrain_xgb\u001b[0;34m(X_train, y_train, X_valid, y_valid)\u001b[0m\n\u001b[1;32m     90\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[1;32m     91\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     92\u001b[0m     tree_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m     colsample_bytree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting XGBoost model on combined protein + genomic features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    103\u001b[0m     X_train,\n\u001b[1;32m    104\u001b[0m     y_train,\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# eval_set=[(X_train, y_train), (X_valid, y_valid)],\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Predict probabilities on valid set\u001b[39;00m\n\u001b[1;32m    110\u001b[0m valid_pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_valid)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xgboost/sklearn.py:1683\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1661\u001b[0m model, metric, params, feature_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[1;32m   1662\u001b[0m     xgb_model, params, feature_weights\n\u001b[1;32m   1663\u001b[0m )\n\u001b[1;32m   1664\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1665\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1666\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1680\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1681\u001b[0m )\n\u001b[0;32m-> 1683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1684\u001b[0m     params,\n\u001b[1;32m   1685\u001b[0m     train_dmatrix,\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m   1687\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[1;32m   1688\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[1;32m   1689\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[1;32m   1690\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[1;32m   1691\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m   1692\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   1693\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1694\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   1695\u001b[0m )\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1698\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xgboost/training.py:183\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xgboost/core.py:2247\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2246\u001b[0m     _check_call(\n\u001b[0;32m-> 2247\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[1;32m   2248\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   2249\u001b[0m         )\n\u001b[1;32m   2250\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2252\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "DATA_DIR = \"/orcd/pool/003/dbertsim_shared/ukb\"\n",
    "outcome_col = \"breast_cancer\"\n",
    "id_col = \"eid\"\n",
    "chrom = \"18\"\n",
    "snp_prefix = chrom\n",
    "olink_prefix = \"olink\"\n",
    "output_prefix = \"ukb_cancer_prot_geno_xgb\"\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # --- Protein data ---\n",
    "    train_prot_path = os.path.join(DATA_DIR, \"ukb_cancer_train.csv\")\n",
    "    valid_prot_path = os.path.join(DATA_DIR, \"ukb_cancer_valid.csv\")\n",
    "\n",
    "    print(f\"Loading protein train from {train_prot_path}\")\n",
    "    train_prot = pd.read_csv(train_prot_path)\n",
    "    print(f\"Loading protein valid from {valid_prot_path}\")\n",
    "    valid_prot = pd.read_csv(valid_prot_path)\n",
    "\n",
    "    # --- Selected variant data ---\n",
    "    train_var_path = f\"ukb_train_{outcome_col}_selected_variants.csv\"\n",
    "    valid_var_path = f\"ukb_valid_{outcome_col}_selected_variants.csv\"\n",
    "\n",
    "    print(f\"Loading variant train from {train_var_path}\")\n",
    "    train_var = pd.read_csv(train_var_path)\n",
    "    print(f\"Loading variant valid from {valid_var_path}\")\n",
    "    valid_var = pd.read_csv(valid_var_path)\n",
    "\n",
    "    # --- Merge on ID column ---\n",
    "    if id_col not in train_prot.columns or id_col not in train_var.columns:\n",
    "        raise ValueError(\n",
    "            f\"ID column '{id_col}' must be present in both protein and variant train data.\"\n",
    "        )\n",
    "    if id_col not in valid_prot.columns or id_col not in valid_var.columns:\n",
    "        raise ValueError(\n",
    "            f\"ID column '{id_col}' must be present in both protein and variant valid data.\"\n",
    "        )\n",
    "\n",
    "    print(f\"Merging train on {id_col}...\")\n",
    "    train_merged = train_prot.merge(train_var, on=[id_col,outcome_col], how=\"inner\")\n",
    "\n",
    "    print(f\"Merging valid on {id_col}...\")\n",
    "    valid_merged = valid_prot.merge(valid_var, on=[id_col,outcome_col], how=\"inner\")\n",
    "\n",
    "    return train_merged, valid_merged\n",
    "\n",
    "\n",
    "def select_features(train_df, valid_df):\n",
    "    # Olink protein features\n",
    "    olink_cols = [c for c in train_df.columns if c.startswith(olink_prefix)]\n",
    "\n",
    "    # SNP features (columns that begin with snp-prefix, e.g. 'c18...', 'c1...')\n",
    "    snp_cols = [c for c in train_df.columns if c.startswith(snp_prefix)]\n",
    "\n",
    "    if len(olink_cols) == 0:\n",
    "        print(\"Warning: no protein columns found with prefix:\", olink_prefix)\n",
    "    if len(snp_cols) == 0:\n",
    "        print(\"Warning: no SNP columns found with prefix:\", snp_prefix)\n",
    "\n",
    "    feature_cols = olink_cols + snp_cols\n",
    "\n",
    "    # Sanity check: make sure outcome is present\n",
    "    if outcome_col not in train_df.columns:\n",
    "        raise ValueError(f\"Outcome column '{outcome_col}' not found in train data.\")\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[outcome_col]\n",
    "\n",
    "    X_valid = valid_df[feature_cols]\n",
    "    y_valid = valid_df[outcome_col]\n",
    "\n",
    "    print(f\"Using {len(feature_cols)} features total:\")\n",
    "    print(f\"  {len(olink_cols)} protein (Olink) features\")\n",
    "    print(f\"  {len(snp_cols)} SNP features\")\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, feature_cols\n",
    "\n",
    "\n",
    "def train_xgb(X_train, y_train, X_valid, y_valid):\n",
    "    # >>> Put your *existing* hyperparameters here to keep them identical <<<\n",
    "    model = XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "        n_estimators=300,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.02,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "    )\n",
    "\n",
    "    print(\"Fitting XGBoost model on combined protein + genomic features...\")\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        # eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Predict probabilities on valid set\n",
    "    valid_pred_proba = model.predict_proba(X_valid)[:, 1]\n",
    "    auc = roc_auc_score(y_valid, valid_pred_proba)\n",
    "    print(f\"Validation AUC: {auc:.4f}\")\n",
    "\n",
    "    return model, valid_pred_proba, auc\n",
    "    \n",
    "# train_merged, valid_merged = load_data()\n",
    "X_train, y_train, X_valid, y_valid, feature_cols = select_features(\n",
    "    train_merged, valid_merged\n",
    ")\n",
    "\n",
    "model, valid_pred_proba, auc = train_xgb(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "# Save predictions & feature list\n",
    "pred_out_path = f\"{output_prefix}_valid_preds.csv\"\n",
    "feat_out_path = f\"{output_prefix}_feature_list.txt\"\n",
    "model_out_path = f\"{output_prefix}_model.json\"\n",
    "\n",
    "print(f\"Saving XGBoost model to {model_out_path}\")\n",
    "model.save_model(model_out_path)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a85f51f2-f33b-4300-a96e-a8d2a686284c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b162c2-3a72-4196-b9e5-5003c2e57f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
